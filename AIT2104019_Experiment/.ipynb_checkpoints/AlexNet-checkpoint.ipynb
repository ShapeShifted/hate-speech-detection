{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQv7TDcA25QP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Thesis/posts1352_standard_representations.csv')"
      ],
      "metadata": {
        "id": "mwH3hogUsh6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import literal_eval\n",
        "\n",
        "token_list = []\n",
        "y_list = []\n",
        "\n",
        "#topic probabilities cannot run because it's integer, representative_docs cannot run because one element\n",
        "#more than one word token\n",
        "# for x in df['Topic']:\n",
        "# for x in df['PreprocessedToken']:\n",
        "for x in df['Topic']:\n",
        "    token_list.append(x)\n",
        "\n",
        "corpus = [token.split() for token in token_list]\n",
        "#bertopic_words = bertopic_words[:1600]\n",
        "\n",
        "for y in df['HateSpeech']:\n",
        "    y_list.append(y)\n",
        "\n",
        "#y_list = y_list[:1600]"
      ],
      "metadata": {
        "id": "cssocyLXLzOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain token list that's actually list instead of string\n",
        "from ast import literal_eval\n",
        "\n",
        "topic_words = []\n",
        "\n",
        "for x in token_list:\n",
        "    topic_words.append((literal_eval(x)))"
      ],
      "metadata": {
        "id": "rzFpc1-UxxOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_strings = [] #(not for topic probabilities)\n",
        "\n",
        "for i in topic_words:\n",
        "\n",
        "  temp = ' '.join(i)\n",
        "  topic_strings.append(temp)"
      ],
      "metadata": {
        "id": "dRBH2U1Q4dNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer #(not for topic probabilities)\n",
        "tokenizer = Tokenizer(num_words=1620)\n",
        "tokenizer.fit_on_texts(topic_strings)\n",
        "sequences = tokenizer.texts_to_sequences(topic_strings)"
      ],
      "metadata": {
        "id": "DyVPypySEK5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad the sequences to have the same length\n",
        "max_sequence_length = 46\n",
        "data = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "data = np.array(data)\n",
        "y_list = np.array(y_list)"
      ],
      "metadata": {
        "id": "sEJkaWGmECqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "\n",
        "cbow_model = gensim.models.Word2Vec.load(\"/content/drive/MyDrive/Thesis/posts1352_standard_cbow_model\")\n",
        "\n",
        "vocab = set(cbow_model.wv.index_to_key)"
      ],
      "metadata": {
        "id": "WqFsiiESF4v5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_words[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2Af_uo08VA4",
        "outputId": "fb058887-a01d-4968-f577-c1d3a0b7fe71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['chinese', 'pigs']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cbow_vector(doc):\n",
        "    # Remove out-of-vocabulary words\n",
        "    doc = [word for word in doc if word in vocab]\n",
        "    return np.mean(cbow_model.wv[doc], axis=0)\n",
        "\n",
        "  # Transform corpus into document vectors\n",
        "doc_vectors = np.array([cbow_vector(doc) for doc in topic_words])"
      ],
      "metadata": {
        "id": "eNx47UWLGBcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "# Load CBOW word embeddings\n",
        "embedding_dim = 100\n",
        "embedding_matrix = doc_vectors\n",
        "\n",
        "# Create the embedding layer\n",
        "embedding_layer = Embedding(input_dim=1620,\n",
        "                            output_dim=embedding_dim,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=max_sequence_length,\n",
        "                            trainable=False)\n"
      ],
      "metadata": {
        "id": "PevtYPnDDe-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "\n",
        "# Define the adapted AlexNet architecture\n",
        "model = Sequential([\n",
        "    embedding_layer,\n",
        "    Conv1D(filters=96, kernel_size=11, strides=1, activation='relu', padding='same'),\n",
        "    MaxPooling1D(pool_size=3, strides=2, padding='same'),\n",
        "    Conv1D(filters=256, kernel_size=5, strides=1, activation='relu', padding='same'),\n",
        "    MaxPooling1D(pool_size=3, strides=2, padding='same'),\n",
        "    Conv1D(filters=384, kernel_size=3, strides=1, activation='relu', padding='same'),\n",
        "    Conv1D(filters=384, kernel_size=3, strides=1, activation='relu', padding='same'),\n",
        "    Conv1D(filters=256, kernel_size=3, strides=1, activation='relu', padding='same'),\n",
        "    MaxPooling1D(pool_size=3, strides=2, padding='same'),\n",
        "    Flatten(),\n",
        "    Dense(4096, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(4096, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1000, activation='sigmoid'),  # Change to number of classes for multi-class classification\n",
        "    Dense(1, activation='sigmoid')  # Change to number of classes for multi-class classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "#None is Batch Size, and is shown on the next code"
      ],
      "metadata": {
        "id": "c0KjT8_bDh3C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a0ca48e-ca13-4992-a2a2-4b140a488b5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, 46, 100)           162000    \n",
            "                                                                 \n",
            " conv1d_55 (Conv1D)          (None, 46, 96)            105696    \n",
            "                                                                 \n",
            " max_pooling1d_33 (MaxPooli  (None, 23, 96)            0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_56 (Conv1D)          (None, 23, 256)           123136    \n",
            "                                                                 \n",
            " max_pooling1d_34 (MaxPooli  (None, 12, 256)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_57 (Conv1D)          (None, 12, 384)           295296    \n",
            "                                                                 \n",
            " conv1d_58 (Conv1D)          (None, 12, 384)           442752    \n",
            "                                                                 \n",
            " conv1d_59 (Conv1D)          (None, 12, 256)           295168    \n",
            "                                                                 \n",
            " max_pooling1d_35 (MaxPooli  (None, 6, 256)            0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " flatten_11 (Flatten)        (None, 1536)              0         \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 4096)              6295552   \n",
            "                                                                 \n",
            " dropout_22 (Dropout)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_40 (Dense)            (None, 4096)              16781312  \n",
            "                                                                 \n",
            " dropout_23 (Dropout)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_41 (Dense)            (None, 1000)              4097000   \n",
            "                                                                 \n",
            " dense_42 (Dense)            (None, 1)                 1001      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 28598913 (109.10 MB)\n",
            "Trainable params: 28436913 (108.48 MB)\n",
            "Non-trainable params: 162000 (632.81 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(data, y_list, epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "id": "D1eNdRgEDjxu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bc12b4f-1c14-4b0a-8db0-07ca824c1638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "41/41 [==============================] - 35s 794ms/step - loss: 0.6867 - accuracy: 0.5833 - val_loss: 0.6956 - val_accuracy: 0.0864\n",
            "Epoch 2/10\n",
            "41/41 [==============================] - 33s 806ms/step - loss: 0.6783 - accuracy: 0.5818 - val_loss: 1.0510 - val_accuracy: 0.0864\n",
            "Epoch 3/10\n",
            "41/41 [==============================] - 30s 737ms/step - loss: 0.6832 - accuracy: 0.5818 - val_loss: 1.1579 - val_accuracy: 0.0864\n",
            "Epoch 4/10\n",
            "41/41 [==============================] - 30s 739ms/step - loss: 0.6832 - accuracy: 0.5995 - val_loss: 0.7678 - val_accuracy: 0.0864\n",
            "Epoch 5/10\n",
            "41/41 [==============================] - 30s 731ms/step - loss: 0.6812 - accuracy: 0.6011 - val_loss: 0.7232 - val_accuracy: 0.0864\n",
            "Epoch 6/10\n",
            "41/41 [==============================] - 33s 801ms/step - loss: 0.6766 - accuracy: 0.5957 - val_loss: 1.1052 - val_accuracy: 0.0864\n",
            "Epoch 7/10\n",
            "41/41 [==============================] - 31s 767ms/step - loss: 0.6927 - accuracy: 0.5764 - val_loss: 0.7246 - val_accuracy: 0.0864\n",
            "Epoch 8/10\n",
            "41/41 [==============================] - 31s 747ms/step - loss: 0.6813 - accuracy: 0.5887 - val_loss: 0.6836 - val_accuracy: 0.9136\n",
            "Epoch 9/10\n",
            "41/41 [==============================] - 30s 739ms/step - loss: 0.6849 - accuracy: 0.5887 - val_loss: 0.9923 - val_accuracy: 0.0864\n",
            "Epoch 10/10\n",
            "41/41 [==============================] - 31s 769ms/step - loss: 0.6842 - accuracy: 0.6003 - val_loss: 0.9583 - val_accuracy: 0.0864\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79c9c03181f0>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model (Topic)\n",
        "loss, accuracy = model.evaluate(data, y_list)\n",
        "print(f'Loss: {loss}, Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "3dqILk4zDmUB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2993bfb9-5ba3-4bf5-a037-36c96cfcabd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51/51 [==============================] - 5s 106ms/step - loss: 0.7316 - accuracy: 0.4981\n",
            "Loss: 0.7315887808799744, Accuracy: 0.4981481432914734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model (PreprocessedToken)\n",
        "loss, accuracy = model.evaluate(data, y_list)\n",
        "print(f'Loss: {loss}, Accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFPRNzPL6FVC",
        "outputId": "d8654e0c-9836-4347-fc8e-c3466647c6e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51/51 [==============================] - 4s 83ms/step - loss: 0.7418 - accuracy: 0.4981\n",
            "Loss: 0.7417582273483276, Accuracy: 0.4981481432914734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/Thesis/posts1352_standard_cbow_alexnet_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRBXWg7AqFtL",
        "outputId": "6d6a26c3-9c77-4c0d-eb7f-3b3493e241b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('/content/drive/MyDrive/Thesis/posts1352_standard_cbow_alexnet_model.h5')"
      ],
      "metadata": {
        "id": "A82SNQcHqhSL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}