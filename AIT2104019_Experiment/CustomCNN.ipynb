{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1eK1DBeOaYZ7-gXq8SK4yHE6IGfoQaUO0","authorship_tag":"ABX9TyNuN3Lg9VC/LRPPt9et1pty"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_52j0v5ByaFG"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","import torch.optim as optim\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","source":["import pandas as pd\n","\n","df = pd.read_csv('/content/drive/MyDrive/Thesis/posts1352_standard_representations.csv')"],"metadata":{"id":"SvnJqHWH6p7C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":597},"id":"xC97x-SvlP0m","executionInfo":{"status":"ok","timestamp":1719965792567,"user_tz":-480,"elapsed":10,"user":{"displayName":"William","userId":"12834679953610585457"}},"outputId":"d3c74a61-04a8-4aa4-8d1a-fa9b366360b5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        No  Category SubCategory  \\\n","0        1      Race     Chinese   \n","1        2      Race      Indian   \n","2        4      Race      Indian   \n","3        5    Gender      Female   \n","4        6      Race       Malay   \n","...    ...       ...         ...   \n","1615  1349  Religion    Buddhism   \n","1616  1350  Religion    Buddhism   \n","1617  1351    Gender        Male   \n","1618  1352    Gender      Female   \n","1619  1352    Gender      Female   \n","\n","                                                   Post  HateSpeech  \\\n","0                                     Chinese are pigs!           1   \n","1            I don't like pajeets they smell like curry           1   \n","2     Indians always smell stinky and weird, what is...           1   \n","3     Women are so emotional they can't handle anyth...           1   \n","4     Malays are dependant on subsidiaries from the ...           1   \n","...                                                 ...         ...   \n","1615  You cannot build a temple in an area with majo...           1   \n","1616  Muslims should not enter temple, or they might...           1   \n","1617  Men that don't have job are useless piece of s...           1   \n","1618                           Women are dick machines.           1   \n","1619    Only want to suck and consume the biggest dick.           1   \n","\n","                                      PreprocessedToken  \\\n","0                                   ['chinese', 'pigs']   \n","1      ['unalike', 'pajeets', 'smell', 'like', 'curry']   \n","2               ['indians', 'smell', 'stinky', 'weird']   \n","3     ['women', 'emotional', 'handle', 'anything', '...   \n","4     ['malays', 'dependant', 'subsidiaries', 'gover...   \n","...                                                 ...   \n","1615  ['build', 'temple', 'area', 'majority', 'musli...   \n","1616  ['muslims', 'exit', 'temple', 'might', 'accide...   \n","1617  ['men', 'lack', 'job', 'useless', 'piece', 'sh...   \n","1618                      ['women', 'dick', 'machines']   \n","1619     ['want', 'suck', 'consume', 'biggest', 'dick']   \n","\n","           Topic_Probabilities  \\\n","0       [4, 0.353802893618629]   \n","1     [7, 0.21967016813460452]   \n","2     [7, 0.20338674496166917]   \n","3                     [1, 1.0]   \n","4                     [2, 1.0]   \n","...                        ...   \n","1615                  [0, 0.0]   \n","1616                 [14, 1.0]   \n","1617                  [1, 1.0]   \n","1618                  [1, 1.0]   \n","1619   [1, 0.9629922744681905]   \n","\n","                                                  Topic  \\\n","0     ['chinese', 'china', 'people', 'pig', 'go', 'i...   \n","1     ['indians', 'indian', 'pajeets', 'smell', 'paj...   \n","2     ['indians', 'indian', 'pajeets', 'smell', 'paj...   \n","3     ['men', 'women', 'woman', 'man', 'sex', 'want'...   \n","4     ['malay', 'malays', 'malaysia', 'malaysian', '...   \n","...                                                 ...   \n","1615  ['disgusting', 'like', 'people', 'middle', 'sy...   \n","1616  ['malaysia', 'buddhists', 'muslims', 'would', ...   \n","1617  ['men', 'women', 'woman', 'man', 'sex', 'want'...   \n","1618  ['men', 'women', 'woman', 'man', 'sex', 'want'...   \n","1619  ['men', 'women', 'woman', 'man', 'sex', 'want'...   \n","\n","                                    Representative_Docs  \n","0     ['chinese pig', 'pig chinese', 'chinese people...  \n","1                ['like indians', 'indians', 'indians']  \n","2                ['like indians', 'indians', 'indians']  \n","3     ['american women literally men', 'men', 'women...  \n","4        ['malays chinese', 'malaysian malay', 'malay']  \n","...                                                 ...  \n","1615  ['well idk rest never encountered problem', 'w...  \n","1616  ['expel buddhists hindus christians malaysia n...  \n","1617  ['american women literally men', 'men', 'women...  \n","1618  ['american women literally men', 'men', 'women...  \n","1619  ['american women literally men', 'men', 'women...  \n","\n","[1620 rows x 9 columns]"],"text/html":["\n","  <div id=\"df-cbbe6151-512b-4783-80a8-c1066708354d\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>No</th>\n","      <th>Category</th>\n","      <th>SubCategory</th>\n","      <th>Post</th>\n","      <th>HateSpeech</th>\n","      <th>PreprocessedToken</th>\n","      <th>Topic_Probabilities</th>\n","      <th>Topic</th>\n","      <th>Representative_Docs</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Race</td>\n","      <td>Chinese</td>\n","      <td>Chinese are pigs!</td>\n","      <td>1</td>\n","      <td>['chinese', 'pigs']</td>\n","      <td>[4, 0.353802893618629]</td>\n","      <td>['chinese', 'china', 'people', 'pig', 'go', 'i...</td>\n","      <td>['chinese pig', 'pig chinese', 'chinese people...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>Race</td>\n","      <td>Indian</td>\n","      <td>I don't like pajeets they smell like curry</td>\n","      <td>1</td>\n","      <td>['unalike', 'pajeets', 'smell', 'like', 'curry']</td>\n","      <td>[7, 0.21967016813460452]</td>\n","      <td>['indians', 'indian', 'pajeets', 'smell', 'paj...</td>\n","      <td>['like indians', 'indians', 'indians']</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4</td>\n","      <td>Race</td>\n","      <td>Indian</td>\n","      <td>Indians always smell stinky and weird, what is...</td>\n","      <td>1</td>\n","      <td>['indians', 'smell', 'stinky', 'weird']</td>\n","      <td>[7, 0.20338674496166917]</td>\n","      <td>['indians', 'indian', 'pajeets', 'smell', 'paj...</td>\n","      <td>['like indians', 'indians', 'indians']</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5</td>\n","      <td>Gender</td>\n","      <td>Female</td>\n","      <td>Women are so emotional they can't handle anyth...</td>\n","      <td>1</td>\n","      <td>['women', 'emotional', 'handle', 'anything', '...</td>\n","      <td>[1, 1.0]</td>\n","      <td>['men', 'women', 'woman', 'man', 'sex', 'want'...</td>\n","      <td>['american women literally men', 'men', 'women...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6</td>\n","      <td>Race</td>\n","      <td>Malay</td>\n","      <td>Malays are dependant on subsidiaries from the ...</td>\n","      <td>1</td>\n","      <td>['malays', 'dependant', 'subsidiaries', 'gover...</td>\n","      <td>[2, 1.0]</td>\n","      <td>['malay', 'malays', 'malaysia', 'malaysian', '...</td>\n","      <td>['malays chinese', 'malaysian malay', 'malay']</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1615</th>\n","      <td>1349</td>\n","      <td>Religion</td>\n","      <td>Buddhism</td>\n","      <td>You cannot build a temple in an area with majo...</td>\n","      <td>1</td>\n","      <td>['build', 'temple', 'area', 'majority', 'musli...</td>\n","      <td>[0, 0.0]</td>\n","      <td>['disgusting', 'like', 'people', 'middle', 'sy...</td>\n","      <td>['well idk rest never encountered problem', 'w...</td>\n","    </tr>\n","    <tr>\n","      <th>1616</th>\n","      <td>1350</td>\n","      <td>Religion</td>\n","      <td>Buddhism</td>\n","      <td>Muslims should not enter temple, or they might...</td>\n","      <td>1</td>\n","      <td>['muslims', 'exit', 'temple', 'might', 'accide...</td>\n","      <td>[14, 1.0]</td>\n","      <td>['malaysia', 'buddhists', 'muslims', 'would', ...</td>\n","      <td>['expel buddhists hindus christians malaysia n...</td>\n","    </tr>\n","    <tr>\n","      <th>1617</th>\n","      <td>1351</td>\n","      <td>Gender</td>\n","      <td>Male</td>\n","      <td>Men that don't have job are useless piece of s...</td>\n","      <td>1</td>\n","      <td>['men', 'lack', 'job', 'useless', 'piece', 'sh...</td>\n","      <td>[1, 1.0]</td>\n","      <td>['men', 'women', 'woman', 'man', 'sex', 'want'...</td>\n","      <td>['american women literally men', 'men', 'women...</td>\n","    </tr>\n","    <tr>\n","      <th>1618</th>\n","      <td>1352</td>\n","      <td>Gender</td>\n","      <td>Female</td>\n","      <td>Women are dick machines.</td>\n","      <td>1</td>\n","      <td>['women', 'dick', 'machines']</td>\n","      <td>[1, 1.0]</td>\n","      <td>['men', 'women', 'woman', 'man', 'sex', 'want'...</td>\n","      <td>['american women literally men', 'men', 'women...</td>\n","    </tr>\n","    <tr>\n","      <th>1619</th>\n","      <td>1352</td>\n","      <td>Gender</td>\n","      <td>Female</td>\n","      <td>Only want to suck and consume the biggest dick.</td>\n","      <td>1</td>\n","      <td>['want', 'suck', 'consume', 'biggest', 'dick']</td>\n","      <td>[1, 0.9629922744681905]</td>\n","      <td>['men', 'women', 'woman', 'man', 'sex', 'want'...</td>\n","      <td>['american women literally men', 'men', 'women...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1620 rows Ã— 9 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cbbe6151-512b-4783-80a8-c1066708354d')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-cbbe6151-512b-4783-80a8-c1066708354d button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-cbbe6151-512b-4783-80a8-c1066708354d');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-1bb09226-4179-47cf-9f47-ceffc279ef52\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1bb09226-4179-47cf-9f47-ceffc279ef52')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-1bb09226-4179-47cf-9f47-ceffc279ef52 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_7ccab39f-08f2-4dde-9d02-270388d8efb1\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_7ccab39f-08f2-4dde-9d02-270388d8efb1 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 1620,\n  \"fields\": [\n    {\n      \"column\": \"No\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 389,\n        \"min\": 1,\n        \"max\": 1352,\n        \"num_unique_values\": 1119,\n        \"samples\": [\n          332,\n          154,\n          1195\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Race\",\n          \"Gender\",\n          \"Religion\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SubCategory\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 14,\n        \"samples\": [\n          \"Male\",\n          \"Hinduism\",\n          \"Chinese\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Post\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1609,\n        \"samples\": [\n          \"Islam people act like they are very holy, but some of them still commit to crime.\",\n          \"Smart man + smart woman = Romance.\",\n          \"Malay all bodoh lah, why do u think they need bumi rights?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HateSpeech\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PreprocessedToken\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1599,\n        \"samples\": [\n          \"['broke', 'trash', 'door', 'chink', 'house']\",\n          \"['happens', 'diff', 'ethnicity', 'racism', 'comes', 'play']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Topic_Probabilities\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 800,\n        \"samples\": [\n          \"[5, 0.7605315353050869]\",\n          \"[4, 0.48806012084294026]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Topic\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 23,\n        \"samples\": [\n          \"['buddhism', 'buddhist', 'buddhists', 'buddha', 'really', 'religion', 'hinduism', 'donated', 'monks', 'new']\",\n          \"['bumi', 'access', 'bifoti', 'housing', 'easier', 'public', 'companies', 'education', 'get', 'really']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Representative_Docs\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 23,\n        \"samples\": [\n          \"['buddhism real religion', 'buddhism really religion', 'theravada buddhism supposed main buddhism buddhist hymns buddhist choirs']\",\n          \"['get access business maybe personal unsure loans certain bumi programs', 'bumi get easier access cheaper tertiary education', 'bumi get access housing discounts bumi affordable housing']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["import gensim.corpora as corpora\n","import gensim.models as models\n","\n","from ast import literal_eval\n","from pprint import pprint\n","\n","bertopic_words = []\n","y_list = []\n","\n","# for x in df['Topic']:\n","# for x in df['Topic_Probabilities']:\n","# for x in df['Representative_Docs']:\n","# for x in df['PreprocessedToken']:\n","for x in df['Representative_Docs']:\n","    bertopic_words.append(x)\n","\n","bertopic_words = bertopic_words[:1600]\n","\n","for y in df['HateSpeech']:\n","    y_list.append(y)\n","\n","y_list = y_list[:1600]"],"metadata":{"id":"OWjG7q-67MNT","executionInfo":{"status":"ok","timestamp":1720553985182,"user_tz":-480,"elapsed":501,"user":{"displayName":"William","userId":"12834679953610585457"}}},"execution_count":114,"outputs":[]},{"cell_type":"code","source":["# obtain token list that's actually list instead of string (ignore if it's topic probabilities)\n","from ast import literal_eval\n","\n","temp = []\n","\n","for x in bertopic_words:\n","    temp.append((literal_eval(x)))\n","\n","bertopic_words = temp"],"metadata":{"id":"V7cA9iOz8nKj","executionInfo":{"status":"ok","timestamp":1720553987272,"user_tz":-480,"elapsed":1,"user":{"displayName":"William","userId":"12834679953610585457"}}},"execution_count":115,"outputs":[]},{"cell_type":"code","source":["# Flatten the list to find unique strings\n","flattened = [item for sublist in bertopic_words for item in sublist]\n","\n","# Create a mapping of each unique string to a unique integer\n","unique_strings = list(set(flattened))\n","string_to_int = {string: i for i, string in enumerate(unique_strings)}\n","\n","# Transform the original list using the mapping\n","converted_list = [[string_to_int[item] for item in sublist] for sublist in bertopic_words]\n","\n","# Normalize the converted list\n","min_val = min(min(sublist) for sublist in converted_list)\n","max_val = max(max(sublist) for sublist in converted_list)\n","\n","normalized_list = [[(item - min_val) / (max_val - min_val) for item in sublist] for sublist in converted_list]"],"metadata":{"id":"5AWjWK5sClj4","executionInfo":{"status":"ok","timestamp":1720553990067,"user_tz":-480,"elapsed":504,"user":{"displayName":"William","userId":"12834679953610585457"}}},"execution_count":116,"outputs":[]},{"cell_type":"markdown","source":["Don't run this if it's PreprocessedToken"],"metadata":{"id":"gSV2N_NyK2Dk"}},{"cell_type":"code","source":["#change into tuple\n","\n","data = []\n","for i in range(len(normalized_list)):\n","  temp_tuple = (normalized_list[i],y_list[i])\n","  data.append(temp_tuple)"],"metadata":{"id":"LyIMIZ2rggoS","executionInfo":{"status":"ok","timestamp":1720553990730,"user_tz":-480,"elapsed":3,"user":{"displayName":"William","userId":"12834679953610585457"}}},"execution_count":117,"outputs":[]},{"cell_type":"markdown","source":["This section is for PreprocessedToken only"],"metadata":{"id":"EOLta0HpKqDe"}},{"cell_type":"code","source":["# Pad the sequences to have the same length ()\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","max_sequence_length = 46\n","padded_data = pad_sequences(converted_list, maxlen=max_sequence_length)\n","#data = np.array(data)\n","#labels = np.array(labels)"],"metadata":{"id":"XGb6vjJMHUWm","executionInfo":{"status":"ok","timestamp":1720553905553,"user_tz":-480,"elapsed":724,"user":{"displayName":"William","userId":"12834679953610585457"}}},"execution_count":97,"outputs":[]},{"cell_type":"code","source":["#change into tuple\n","\n","data = []\n","for i in range(len(converted_list)):\n","  temp_tuple = (padded_data[i],y_list[i])\n","  data.append(temp_tuple)"],"metadata":{"id":"d2hdLrSGILCG","executionInfo":{"status":"ok","timestamp":1720553907953,"user_tz":-480,"elapsed":1,"user":{"displayName":"William","userId":"12834679953610585457"}}},"execution_count":98,"outputs":[]},{"cell_type":"markdown","source":["Continue back to the public section"],"metadata":{"id":"yRaOicM-KyKc"}},{"cell_type":"code","source":["num_epochs = 10\n","batch_size = 32\n","learning_rate = 0.001"],"metadata":{"id":"MgXLsTE7XO5L","executionInfo":{"status":"ok","timestamp":1720553993043,"user_tz":-480,"elapsed":514,"user":{"displayName":"William","userId":"12834679953610585457"}}},"execution_count":118,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        x, y = self.data[idx]\n","        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n","\n","dataset = CustomDataset(data)\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"],"metadata":{"id":"K79LgVa2sj5k","executionInfo":{"status":"ok","timestamp":1720553994427,"user_tz":-480,"elapsed":552,"user":{"displayName":"William","userId":"12834679953610585457"}}},"execution_count":119,"outputs":[]},{"cell_type":"code","source":["#Topic\n","\n","class ConvNet(nn.Module):\n","    def __init__(self):\n","        super(ConvNet, self).__init__()\n","        self.conv1 = nn.Conv1d(1, 4, 3) # default stride = 1\n","        # channels of input, channels of output, filter size: 3 x 3\n","        self.conv2 = nn.Conv1d(4, 8, 3)\n","        self.conv3 = nn.Conv1d(8, 16, 3)\n","        self.conv4 = nn.Conv1d(16, 32, 3)\n","        self.pool = nn.MaxPool1d(kernel_size=2)\n","        self.fc1 = nn.Linear(32*2, 32)\n","        self.fc2 = nn.Linear(32, 16)\n","        self.fc3 = nn.Linear(16, 2)\n","        self.dropout = nn.Dropout(0.5)\n","\n","\n","    def forward(self, x):\n","        # -> batch size, 10\n","        x = x.unsqueeze(1)  # batch size, 1 (channel), 10 (length of channel)\n","        x = self.conv1(x) # batch size, 4, (10-3)/1 + 1 = 8\n","        x = F.relu(x)\n","        x = self.dropout(x)\n","        x = self.conv2(x) # batch size, 8, (8-3)+1 = 6\n","        x = F.relu(x)\n","        x = self.dropout(x)\n","        x = self.conv3(x) #batch size, 16, (6-3)+1 = 4\n","        x = F.relu(x)\n","        x = self.dropout(x)\n","        x = self.conv4(x) #batch size, 32, (4-3)+1 = 2\n","        x = F.relu(x)\n","        x = self.dropout(x)\n","        x = x.view(-1, 32*2)\n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        x = self.fc2(x)\n","        x = F.relu(x)\n","        x = self.fc3(x)\n","\n","\n","        return x"],"metadata":{"id":"qDY0Fqy1RTtL","executionInfo":{"status":"ok","timestamp":1720553956475,"user_tz":-480,"elapsed":2,"user":{"displayName":"William","userId":"12834679953610585457"}}},"execution_count":110,"outputs":[]},{"cell_type":"code","source":["#Topic Probabilities\n","\n","class ConvNet(nn.Module):\n","    def __init__(self):\n","        super(ConvNet, self).__init__()\n","        self.conv1 = nn.Conv1d(1, 4, 1) # default stride = 1\n","        # channels of input, channels of output, filter size: 2 x 2\n","        self.conv2 = nn.Conv1d(4, 8, 1)\n","        self.conv3 = nn.Conv1d(8, 16, 1)\n","        self.conv4 = nn.Conv1d(16, 32, 1)\n","        self.pool = nn.MaxPool1d(kernel_size=2)\n","        self.fc1 = nn.Linear(32*2, 32)\n","        self.fc2 = nn.Linear(32, 16)\n","        self.fc3 = nn.Linear(16, 2)\n","        self.dropout = nn.Dropout(0.5)\n","\n","\n","    def forward(self, x):\n","        # -> batch size, 10\n","        x = x.unsqueeze(1)  # batch size, 1 (channel), 2 (length of channel)\n","        x = self.conv1(x) # batch size, 4, (2-1)/1 + 1 = 2\n","        x = F.relu(x)\n","        x = self.dropout(x)\n","        x = self.conv2(x) # batch size, 8, (2-1)/1 + 1 = 2\n","        x = F.relu(x)\n","        x = self.dropout(x)\n","        x = self.conv3(x) #batch size, 16, (2-1)/1 + 1 = 2\n","        x = F.relu(x)\n","        x = self.dropout(x)\n","        x = self.conv4(x) #batch size, 32, (2-1)/1 + 1 = 2\n","        x = F.relu(x)\n","        x = self.dropout(x)\n","        x = x.view(-1, 32*2)\n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        x = self.fc2(x)\n","        x = F.relu(x)\n","        x = self.fc3(x)\n","\n","\n","        return x"],"metadata":{"id":"_iiyCX1d9eJ0","executionInfo":{"status":"ok","timestamp":1720553959021,"user_tz":-480,"elapsed":1,"user":{"displayName":"William","userId":"12834679953610585457"}}},"execution_count":111,"outputs":[]},{"cell_type":"code","source":["#Representative Doc\n","\n","class ConvNet(nn.Module):\n","    def __init__(self):\n","        super(ConvNet, self).__init__()\n","        self.conv1 = nn.Conv1d(1, 4, 1) # default stride = 1\n","        # channels of input, channels of output, filter size: 2 x 2\n","        self.conv2 = nn.Conv1d(4, 8, 2)\n","        self.conv3 = nn.Conv1d(8, 16, 1)\n","        self.conv4 = nn.Conv1d(16, 32, 1)\n","        self.pool = nn.MaxPool1d(kernel_size=2)\n","        self.fc1 = nn.Linear(32*2, 32)\n","        self.fc2 = nn.Linear(32, 16)\n","        self.fc3 = nn.Linear(16, 2)\n","        self.dropout = nn.Dropout(0.5)\n","\n","\n","    def forward(self, x):\n","        # -> batch size, 10\n","        x = x.unsqueeze(1)  # batch size, 1 (channel), 3 (length of channel)\n","        x = self.conv1(x) # batch size, 4, (3-1)/1 + 1 = 3\n","        x = F.relu(x)\n","        x = self.dropout(x)\n","        x = self.conv2(x) # batch size, 8, (3-2)/1 + 1 = 2\n","        x = F.relu(x)\n","        x = self.dropout(x)\n","        x = self.conv3(x) #batch size, 16, (2-1)/1 + 1 = 2\n","        x = F.relu(x)\n","        x = self.dropout(x)\n","        x = self.conv4(x) #batch size, 32, (2-1)/1 + 1 = 2\n","        x = F.relu(x)\n","        x = self.dropout(x)\n","        x = x.view(-1, 32*2)\n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        x = self.fc2(x)\n","        x = F.relu(x)\n","        x = self.fc3(x)\n","\n","\n","        return x"],"metadata":{"id":"L3aAt94h-H62","executionInfo":{"status":"ok","timestamp":1720553996740,"user_tz":-480,"elapsed":1,"user":{"displayName":"William","userId":"12834679953610585457"}}},"execution_count":120,"outputs":[]},{"cell_type":"code","source":["#PreprocessedToken\n","\n","class ConvNet(nn.Module):\n","    def __init__(self):\n","        super(ConvNet, self).__init__()\n","        self.conv1 = nn.Conv1d(1, 4, 15) # default stride = 1\n","        # channels of input, channels of output, filter size: 2 x 2\n","        self.conv2 = nn.Conv1d(4, 8, 17)\n","        self.conv3 = nn.Conv1d(8, 16, 9)\n","        self.conv4 = nn.Conv1d(16, 32, 7)\n","        self.pool = nn.MaxPool1d(kernel_size=2)\n","        self.fc1 = nn.Linear(32*2, 32)\n","        self.fc2 = nn.Linear(32, 16)\n","        self.fc3 = nn.Linear(16, 2)\n","        self.dropout = nn.Dropout(0.5)\n","\n","\n","    def forward(self, x):\n","        # -> batch size, 46\n","        x = x.unsqueeze(1)  # batch size, 1 (channel), 8 (length of channel)\n","        x = self.conv1(x) # batch size, 4, (46-15)/1 + 1 = 32\n","        x = F.relu(x)\n","        x = self.dropout(x)\n","        x = self.conv2(x) # batch size, 8, (32-17)/1 + 1 = 16\n","        x = F.relu(x)\n","        x = self.dropout(x)\n","        x = self.conv3(x) #batch size, 16, (16-9)/1 + 1 = 8\n","        x = F.relu(x)\n","        x = self.dropout(x)\n","        x = self.conv4(x) #batch size, 32, (8-7)/1 + 1 = 2\n","        x = F.relu(x)\n","        x = self.dropout(x)\n","        x = x.view(-1, 32*2)\n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        x = self.fc2(x)\n","        x = F.relu(x)\n","        x = self.fc3(x)\n","\n","\n","        return x"],"metadata":{"id":"jZ61bU35Jg2L","executionInfo":{"status":"ok","timestamp":1720553916128,"user_tz":-480,"elapsed":3,"user":{"displayName":"William","userId":"12834679953610585457"}}},"execution_count":101,"outputs":[]},{"cell_type":"code","source":["model = ConvNet()\n","loss = nn.CrossEntropyLoss() #final activation function is already in here\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","n_total_steps = len(dataloader)\n","i = 0\n","\n","#remember to set the correct data first\n","for epoch in range(num_epochs):\n","    model.train()\n","    for images, labels in dataloader:\n","        # Forward function\n","        outputs = model(images)\n","        labels= labels.long()\n","        l = loss(outputs, labels)\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        l.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","\n","    print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {l.item():.4f}')\n","print('Finished Training')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wX7KR9BYKmH7","executionInfo":{"status":"ok","timestamp":1720554004151,"user_tz":-480,"elapsed":3582,"user":{"displayName":"William","userId":"12834679953610585457"}},"outputId":"f2cfe63f-7b90-41fe-f7f7-6e71a02a606d"},"execution_count":121,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/10], Loss: 0.6942\n","Epoch [2/10], Loss: 0.7000\n","Epoch [3/10], Loss: 0.6919\n","Epoch [4/10], Loss: 0.6918\n","Epoch [5/10], Loss: 0.6966\n","Epoch [6/10], Loss: 0.6943\n","Epoch [7/10], Loss: 0.6928\n","Epoch [8/10], Loss: 0.6989\n","Epoch [9/10], Loss: 0.6931\n","Epoch [10/10], Loss: 0.6936\n","Finished Training\n"]}]},{"cell_type":"code","source":["# Evaluate the model (Topic)\n","model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for inputs, labels in dataloader:\n","        outputs = model(inputs)\n","        max_value, predictions = torch.max(outputs, 1)\n","        correct += (predictions == labels).sum().item()\n","        total += labels.size(0)\n","\n","accuracy = correct / total\n","print(f'Accuracy: {accuracy:.2f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3AZbHhaB6EYx","executionInfo":{"status":"ok","timestamp":1720553875407,"user_tz":-480,"elapsed":498,"user":{"displayName":"William","userId":"12834679953610585457"}},"outputId":"913e4701-8f5a-476c-b801-bc9359ca7fdf"},"execution_count":93,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.51\n"]}]},{"cell_type":"code","source":["# Evaluate the model (Topic Probabilities)\n","model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for inputs, labels in dataloader:\n","        outputs = model(inputs)\n","        max_value, predictions = torch.max(outputs, 1)\n","        correct += (predictions == labels).sum().item()\n","        total += labels.size(0)\n","\n","accuracy = correct / total\n","print(f'Accuracy: {accuracy:.2f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O--v1JHU8rFy","executionInfo":{"status":"ok","timestamp":1720553970268,"user_tz":-480,"elapsed":518,"user":{"displayName":"William","userId":"12834679953610585457"}},"outputId":"c8a539c4-257e-4277-a1c5-ca60eac6e4ae"},"execution_count":113,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.51\n"]}]},{"cell_type":"code","source":["# Evaluate the model (Representative Documents)\n","model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for inputs, labels in dataloader:\n","        outputs = model(inputs)\n","        max_value, predictions = torch.max(outputs, 1)\n","        correct += (predictions == labels).sum().item()\n","        total += labels.size(0)\n","\n","accuracy = correct / total\n","print(f'Accuracy: {accuracy:.2f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kx96ueQu8xQS","executionInfo":{"status":"ok","timestamp":1720554004152,"user_tz":-480,"elapsed":4,"user":{"displayName":"William","userId":"12834679953610585457"}},"outputId":"93d0b8f6-0425-4428-c82b-74d6f2ce7c70"},"execution_count":122,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.49\n"]}]},{"cell_type":"code","source":["# Evaluate the model (PreprocessedToken)\n","model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for inputs, labels in dataloader:\n","        outputs = model(inputs)\n","        max_value, predictions = torch.max(outputs, 1)\n","        correct += (predictions == labels).sum().item()\n","        total += labels.size(0)\n","\n","accuracy = correct / total\n","print(f'Accuracy: {accuracy:.2f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nwP-SmqcKKuc","executionInfo":{"status":"ok","timestamp":1720553929223,"user_tz":-480,"elapsed":825,"user":{"displayName":"William","userId":"12834679953610585457"}},"outputId":"fdc85b31-5968-469a-aa28-bc99817c0e67"},"execution_count":103,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.54\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), '/content/drive/MyDrive/Thesis/posts1352_standard_cnn_model.pth')\n","\n","# Create a new instance of the model\n","model = ConvNet()\n","\n","# Load the saved state dictionary\n","model.load_state_dict(torch.load('/content/drive/MyDrive/Thesis/posts1352_standard_cnn_model.pth'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cnmwSt-erOHv","executionInfo":{"status":"ok","timestamp":1720112312809,"user_tz":-480,"elapsed":2,"user":{"displayName":"William","userId":"12834679953610585457"}},"outputId":"f7a964b5-de9b-4a1c-97f2-9cf6a710bf30"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":70}]},{"cell_type":"code","source":[],"metadata":{"id":"wKToN2H2QrR8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["CNN model training on CBOW embeddings"],"metadata":{"id":"u0_zuLEprO7w"}},{"cell_type":"code","source":["import gensim\n","\n","cbow_model = gensim.models.Word2Vec.load(\"/content/drive/MyDrive/Thesis/posts1352_standard_cbow_model\")\n"],"metadata":{"id":"VQqXPV2zY9E1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from ast import literal_eval\n","\n","tokenlist = []\n","for x in df['PreprocessedToken']:\n","    tokenlist.append(' '.join(literal_eval(x)))\n","\n","corpus = [token.split() for token in tokenlist]\n","\n","vocab = set(cbow_model.wv.index_to_key)"],"metadata":{"id":"FQgjNX3WcQEU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = []\n","\n","for y in df['HateSpeech']:\n","    labels.append(y)"],"metadata":{"id":"a8VX92jvjFxs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","def cbow_vector(doc):\n","    # Remove out-of-vocabulary words\n","    doc = [word for word in doc if word in vocab]\n","    return np.mean(cbow_model.wv[doc], axis=0)\n","\n","  # Transform corpus into document vectors\n","doc_vectors = np.array([cbow_vector(doc) for doc in corpus])"],"metadata":{"id":"mma9Tx3EbI1t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate the number of full batches of size (32, 100)\n","batch_size = 32\n","num_full_batches = doc_vectors.shape[0] // batch_size\n","\n","# Remove the last part that's not a full batch\n","doc_vectors = doc_vectors[:num_full_batches * batch_size]\n","labels = labels[:num_full_batches * batch_size]\n","\n","# Reshape the array into batches of shape (32, 100)\n","batches = doc_vectors.reshape(-1, batch_size, doc_vectors.shape[1])\n","batched_labels = [labels[i:i + batch_size] for i in range(0, len(labels), batch_size)]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dH-6W7wKf1Nx","executionInfo":{"status":"ok","timestamp":1720110996986,"user_tz":-480,"elapsed":386,"user":{"displayName":"William","userId":"12834679953610585457"}},"outputId":"f979b5bc-c73e-42c0-e63d-f366048c0f12"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["50\n","32\n"]}]},{"cell_type":"code","source":["class CNNConvNet(nn.Module):\n","    def __init__(self):\n","        super(CNNConvNet, self).__init__()\n","        self.conv1 = nn.Conv1d(1, 4, 8) # default stride = 1\n","        # channels of input, channels of output, filter size: 2 x 2\n","        self.conv2 = nn.Conv1d(4, 8, 9)\n","        self.conv3 = nn.Conv1d(8, 16, 6)\n","        self.conv4 = nn.Conv1d(16, 32, 4)\n","        self.pool = nn.MaxPool1d(kernel_size=2)\n","        self.fc1 = nn.Linear(32*2, 32)\n","        self.fc2 = nn.Linear(32, 16)\n","        self.fc3 = nn.Linear(16, 2)\n","        self.dropout = nn.Dropout(0.5)\n","        self.pooling = nn.MaxPool1d(kernel_size = 2)\n","\n","\n","    def forward(self, x):\n","        # -> batch size, 100\n","        x = x.unsqueeze(1)  # batch size, 1 (channel), 100 (length of channel)\n","        x = self.conv1(x) # batch size, 4, (100-9)/1 + 1 = 92\n","        x = F.relu(x)\n","        x = self.pooling(x) #batch size, 4, 92/2 = 46\n","        x = self.dropout(x)\n","        x = self.conv2(x) # batch size, 8, (46-9)+1 = 38\n","        x = F.relu(x)\n","        x = self.pooling(x) #batch size, 8, 38/2 = 19\n","        x = self.dropout(x)\n","        x = self.conv3(x) #batch size, 16, (19-6)+1 = 14\n","        x = F.relu(x)\n","        x = self.pooling(x)#batch size, 16, 14/2 = 7\n","        x = self.dropout(x)\n","        x = self.conv4(x) #batch size, 32, (7-4)+1 = 4\n","        x = F.relu(x)\n","        x = self.pooling(x) #batch size, 32, 4/2 = 2\n","        x = self.dropout(x)\n","        x = x.view(-1, 32*2)\n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        x = self.fc2(x)\n","        x = F.relu(x)\n","        x = self.fc3(x)\n","\n","\n","        return x"],"metadata":{"id":"hhr5AyX_g6nS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = CNNConvNet()\n","loss = nn.CrossEntropyLoss() #final activatioh function is already in here\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","i = 0\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    for batch , labels in zip(batches, batched_labels):\n","        # Forward function\n","        batch = torch.from_numpy(batch)\n","        outputs = model(batch)\n","        labels = torch.tensor(labels)\n","        l = loss(outputs, labels)\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        l.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","\n","    print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {l.item():.4f}')\n","print('Finished Training')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aDteafv5i4Wo","executionInfo":{"status":"ok","timestamp":1720111144613,"user_tz":-480,"elapsed":5432,"user":{"displayName":"William","userId":"12834679953610585457"}},"outputId":"65e1c546-271f-49d6-94f7-0b567dcf3469"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/10], Loss: 0.7392\n","Epoch [2/10], Loss: 0.7249\n","Epoch [3/10], Loss: 0.7159\n","Epoch [4/10], Loss: 0.7103\n","Epoch [5/10], Loss: 0.7068\n","Epoch [6/10], Loss: 0.7037\n","Epoch [7/10], Loss: 0.7018\n","Epoch [8/10], Loss: 0.7000\n","Epoch [9/10], Loss: 0.6981\n","Epoch [10/10], Loss: 0.6980\n","Finished Training\n"]}]},{"cell_type":"code","source":["# Evaluate the model (CBOW)\n","model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for batch , labels in zip(batches, batched_labels):\n","        batch = torch.from_numpy(batch)\n","        outputs = model(batch)\n","        labels = torch.tensor(labels)\n","        max_value, predictions = torch.max(outputs, 1)\n","        correct += (predictions == labels).sum().item()\n","        total += labels.size(0)\n","\n","accuracy = correct / total\n","print(f'Accuracy: {accuracy:.2f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nZo1y1uqj0GL","executionInfo":{"status":"ok","timestamp":1720111285785,"user_tz":-480,"elapsed":601,"user":{"displayName":"William","userId":"12834679953610585457"}},"outputId":"f9790f31-68e8-466f-b4d0-1089311a7662"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.49\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), '/content/drive/MyDrive/Thesis/posts1352_standard_cbow_cnn_model.pth')\n","\n","# Create a new instance of the model\n","model = CNNConvNet()\n","\n","# Load the saved state dictionary\n","model.load_state_dict(torch.load('/content/drive/MyDrive/Thesis/posts1352_standard_cbow_cnn_model.pth'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SilWiHlVqq4B","executionInfo":{"status":"ok","timestamp":1720112156912,"user_tz":-480,"elapsed":413,"user":{"displayName":"William","userId":"12834679953610585457"}},"outputId":"43a32ee1-3c6d-445d-d778-b696f199a0be"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":58}]}]}