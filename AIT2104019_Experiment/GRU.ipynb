{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 14235,
     "status": "ok",
     "timestamp": 1720588046178,
     "user": {
      "displayName": "William",
      "userId": "12834679953610585457"
     },
     "user_tz": -480
    },
    "id": "OxiKx8uuHTeZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 628,
     "status": "ok",
     "timestamp": 1720588046805,
     "user": {
      "displayName": "William",
      "userId": "12834679953610585457"
     },
     "user_tz": -480
    },
    "id": "d-VRbgMzL_81"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/content/drive/MyDrive/Thesis/posts1352_standard_representations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1720588386652,
     "user": {
      "displayName": "William",
      "userId": "12834679953610585457"
     },
     "user_tz": -480
    },
    "id": "1de1Lv1TMGW-"
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "token_list = []\n",
    "labels = []\n",
    "\n",
    "# for x in df['Topic_Probabilities']:\n",
    "# for x in df['Topic']:\n",
    "# for x in df['Representative_Docs']:\n",
    "# for x in df['PreprocessedToken']:\n",
    "for x in df['Topic']:\n",
    "    token_list.append(x)\n",
    "\n",
    "corpus = [token.split() for token in token_list]\n",
    "#bertopic_words = bertopic_words[:1600]\n",
    "\n",
    "for y in df['HateSpeech']:\n",
    "    labels.append(y)\n",
    "\n",
    "#y_list = y_list[:1600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1720588388184,
     "user": {
      "displayName": "William",
      "userId": "12834679953610585457"
     },
     "user_tz": -480
    },
    "id": "YMxGZ2jYMJQM"
   },
   "outputs": [],
   "source": [
    "# obtain token list that's actually list instead of string\n",
    "from ast import literal_eval\n",
    "\n",
    "topic_words = []\n",
    "\n",
    "for x in token_list:\n",
    "    topic_words.append((literal_eval(x)))\n",
    "\n",
    "#sequences = topic_words #only for topic probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1720588388622,
     "user": {
      "displayName": "William",
      "userId": "12834679953610585457"
     },
     "user_tz": -480
    },
    "id": "QJk_Ttg8MKq8"
   },
   "outputs": [],
   "source": [
    "# change the list containing a list of elements into list of strings (not for topic probabilities)\n",
    "topic_strings = []\n",
    "\n",
    "for i in topic_words:\n",
    "\n",
    "  temp = ' '.join(i)\n",
    "  topic_strings.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1720588390331,
     "user": {
      "displayName": "William",
      "userId": "12834679953610585457"
     },
     "user_tz": -480
    },
    "id": "GiAxGTSRMMt2"
   },
   "outputs": [],
   "source": [
    "# Initialize the tokenizer (not for topic probabilities)\n",
    "tokenizer = Tokenizer(num_words=1620)\n",
    "tokenizer.fit_on_texts(topic_strings)\n",
    "sequences = tokenizer.texts_to_sequences(topic_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7tIGjD-kB3JW"
   },
   "outputs": [],
   "source": [
    "# Save the tokenizer to a file\n",
    "with open('/content/drive/MyDrive/Thesis/posts1352_standard_GRU_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sYfYinOfB7VG"
   },
   "outputs": [],
   "source": [
    "# Load the tokenizer from the file\n",
    "with open('/content/drive/MyDrive/Thesis/posts1352_standard_GRU_tokenizer.pickle', 'rb') as handle:\n",
    "    loaded_tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1720588391752,
     "user": {
      "displayName": "William",
      "userId": "12834679953610585457"
     },
     "user_tz": -480
    },
    "id": "672z8m8NMR7U"
   },
   "outputs": [],
   "source": [
    "# Pad the sequences to have the same length\n",
    "max_sequence_length = 46\n",
    "data = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1720588392740,
     "user": {
      "displayName": "William",
      "userId": "12834679953610585457"
     },
     "user_tz": -480
    },
    "id": "qVr4wxeEMZ1U"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 937,
     "status": "ok",
     "timestamp": 1720588394892,
     "user": {
      "displayName": "William",
      "userId": "12834679953610585457"
     },
     "user_tz": -480
    },
    "id": "75x3vPboMh4n"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=1620, output_dim=128, input_length=max_sequence_length),\n",
    "    GRU(units=128, return_sequences=True),\n",
    "    GRU(units=128),\n",
    "    Dropout(0.5),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1720588396836,
     "user": {
      "displayName": "William",
      "userId": "12834679953610585457"
     },
     "user_tz": -480
    },
    "id": "XPdtQ_jjMiRY",
    "outputId": "e65f942c-75c2-48cd-e4c9-20e7c9719810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 46, 128)           207360    \n",
      "                                                                 \n",
      " gru_4 (GRU)                 (None, 46, 128)           99072     \n",
      "                                                                 \n",
      " gru_5 (GRU)                 (None, 128)               99072     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 413825 (1.58 MB)\n",
      "Trainable params: 413825 (1.58 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 87933,
     "status": "ok",
     "timestamp": 1720588486530,
     "user": {
      "displayName": "William",
      "userId": "12834679953610585457"
     },
     "user_tz": -480
    },
    "id": "9BTs_KnzM1sO",
    "outputId": "a73e68e4-7ed8-42e0-ac8e-e50a4243dba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "41/41 [==============================] - 12s 145ms/step - loss: 0.6929 - accuracy: 0.5278 - val_loss: 0.6786 - val_accuracy: 0.5802\n",
      "Epoch 2/10\n",
      "41/41 [==============================] - 8s 188ms/step - loss: 0.6875 - accuracy: 0.5409 - val_loss: 0.6707 - val_accuracy: 0.5833\n",
      "Epoch 3/10\n",
      "41/41 [==============================] - 5s 123ms/step - loss: 0.6856 - accuracy: 0.5409 - val_loss: 0.6738 - val_accuracy: 0.5648\n",
      "Epoch 4/10\n",
      "41/41 [==============================] - 5s 125ms/step - loss: 0.6799 - accuracy: 0.5718 - val_loss: 0.6689 - val_accuracy: 0.5864\n",
      "Epoch 5/10\n",
      "41/41 [==============================] - 7s 179ms/step - loss: 0.6819 - accuracy: 0.5640 - val_loss: 0.6617 - val_accuracy: 0.5833\n",
      "Epoch 6/10\n",
      "41/41 [==============================] - 5s 124ms/step - loss: 0.6782 - accuracy: 0.5525 - val_loss: 0.6640 - val_accuracy: 0.5710\n",
      "Epoch 7/10\n",
      "41/41 [==============================] - 7s 173ms/step - loss: 0.6745 - accuracy: 0.5656 - val_loss: 0.6646 - val_accuracy: 0.5802\n",
      "Epoch 8/10\n",
      "41/41 [==============================] - 5s 124ms/step - loss: 0.6746 - accuracy: 0.5694 - val_loss: 0.6600 - val_accuracy: 0.5926\n",
      "Epoch 9/10\n",
      "41/41 [==============================] - 5s 123ms/step - loss: 0.6757 - accuracy: 0.5579 - val_loss: 0.6610 - val_accuracy: 0.5957\n",
      "Epoch 10/10\n",
      "41/41 [==============================] - 8s 185ms/step - loss: 0.6785 - accuracy: 0.5702 - val_loss: 0.6638 - val_accuracy: 0.5864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x794985df5630>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 955,
     "status": "ok",
     "timestamp": 1720588495838,
     "user": {
      "displayName": "William",
      "userId": "12834679953610585457"
     },
     "user_tz": -480
    },
    "id": "-4CuoxrLQPbo",
    "outputId": "1273c09e-f5a7-4de4-9a56-008a584ecaa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 29ms/step - loss: 0.6638 - accuracy: 0.5864\n",
      "Validation Loss: 0.6638393402099609\n",
      "Validation Accuracy: 0.5864197611808777\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation data of Topic\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Loss: {loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1023,
     "status": "ok",
     "timestamp": 1720588179656,
     "user": {
      "displayName": "William",
      "userId": "12834679953610585457"
     },
     "user_tz": -480
    },
    "id": "RtAhfx5X7TOh",
    "outputId": "a6cd502c-ab22-4c91-9177-b766011f7c08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 1s 55ms/step - loss: 0.6755 - accuracy: 0.5401\n",
      "Validation Loss: 0.6754544973373413\n",
      "Validation Accuracy: 0.540123462677002\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation data of Topic Probability\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Loss: {loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1040,
     "status": "ok",
     "timestamp": 1720588367283,
     "user": {
      "displayName": "William",
      "userId": "12834679953610585457"
     },
     "user_tz": -480
    },
    "id": "ZylgkL4e7VCw",
    "outputId": "9a27dfb5-ca12-40cf-c3c7-cd5dffa937e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 28ms/step - loss: 0.6859 - accuracy: 0.5247\n",
      "Validation Loss: 0.6858827471733093\n",
      "Validation Accuracy: 0.5246913433074951\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation data of Representative Document\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Loss: {loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1023,
     "status": "ok",
     "timestamp": 1720248795661,
     "user": {
      "displayName": "William",
      "userId": "12834679953610585457"
     },
     "user_tz": -480
    },
    "id": "TevXYxrmzcWc",
    "outputId": "fdd6d470-cab3-497b-8858-547682dd2f5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 38ms/step - loss: 1.0015 - accuracy: 0.8519\n",
      "Validation Loss: 1.0015413761138916\n",
      "Validation Accuracy: 0.8518518805503845\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation data of WordRepresentation\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Loss: {loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1720248881346,
     "user": {
      "displayName": "William",
      "userId": "12834679953610585457"
     },
     "user_tz": -480
    },
    "id": "QjEelDgsM3jk",
    "outputId": "1f35b51c-5463-483e-8dc3-82c946f6114c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('/content/drive/MyDrive/Thesis/posts1352_standard_GRU_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ILjz37o_0mLz"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('/content/drive/MyDrive/Thesis/posts1352_standard_GRU_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstring\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtextblob\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wordnet\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# %load preprocess.py\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "stopword_extra = [\"always\",\"want\",\"even\",\"still\",\"ever\",\"also\",\"already\",\"yet\",\"basically\",\"actually\",\"need\",\"please\",\"ago\",\"probable\",\"probably\",\"however\",\"instead\",\"quite\",\"nt\",\"na\",\"u\",\"gon\",\"lol\",\"im\",\"ca\",\"us\",\"cnt\",\"wo\",\"em\",\"etc\",\"ll\",\"aint\",\"r\",\"cant\",\"shouldnt\",\"wont\",\"lah\",\"dont\",\"never\"]\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords += stopword_extra\n",
    "\n",
    "negative_list = ['not','never','ain','aint','no','neither','nor','nt','cant','dont',\"cnt\",'wont',\"shouldnt\"]\n",
    "\n",
    "def preprocess(input):\n",
    "    preprocessed_input = []\n",
    "\n",
    "    input = sent_tokenize(input)\n",
    "\n",
    "    for text in input:\n",
    "\n",
    "        #1. Generating the list of words in the tweet (hastags and other punctuations removed)\n",
    "        text_blob = TextBlob(text)\n",
    "        text = ' '.join(text_blob.words)\n",
    "\n",
    "        # remove number\n",
    "        text = re.sub(r'[0-9]', '', text)\n",
    "\n",
    "        # lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        text = text.replace('/',' ')\n",
    "\n",
    "        for punctuation in string.punctuation:\n",
    "            text = text.replace(punctuation, '')\n",
    "\n",
    "        text = word_tokenize(text)\n",
    "\n",
    "        #keep tokens that are alphabet characters\n",
    "        text = [t for t in text if t.isalpha()]\n",
    "\n",
    "        # replace the negation token\n",
    "        replacer  = AntonymReplacer()\n",
    "        text = replacer.replace_negations(text)\n",
    "\n",
    "        # remove the stopwords\n",
    "        text = [i for i in text if i not in stopwords]\n",
    "\n",
    "        #remove empty token\n",
    "        text = [t for t in text if len(t) > 0]\n",
    "\n",
    "        preprocessed_input.append(text)\n",
    "\n",
    "\n",
    "    return preprocessed_input\n",
    "\n",
    "\n",
    "def lemmatization(sent, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    doc = nlp(\" \".join(sent))\n",
    "    texts_out = [token.lemma_ for token in doc if token.pos_ in allowed_postags]\n",
    "    return texts_out\n",
    "\n",
    "class AntonymReplacer(object):\n",
    "    def replace(self, word, pos=None):\n",
    "        antonyms = set()\n",
    "\n",
    "        for syn in wordnet.synsets(word, pos=pos):\n",
    "            for lemma in syn.lemmas():\n",
    "                for antonym in lemma.antonyms():\n",
    "                    antonyms.add(antonym.name())\n",
    "\n",
    "        if len(antonyms) >= 1:\n",
    "            return antonyms.pop()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def replace_negations(self, sent):\n",
    "        i, l = 0, len(sent)\n",
    "        words = []\n",
    "\n",
    "        while i < l:\n",
    "            word = sent[i]\n",
    "\n",
    "            if word in negative_list and i+1 < l:\n",
    "                ant = self.replace(sent[i+1])\n",
    "\n",
    "                if ant:\n",
    "                    words.append(ant)\n",
    "                    i += 2\n",
    "                    continue\n",
    "\n",
    "            words.append(word)\n",
    "            i += 1\n",
    "\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load prediction.py\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import keras\n",
    "\n",
    "max_sequence_length = 46\n",
    "\n",
    "def change_to_string(input):\n",
    "    # obtain token list that's actually list instead of string\n",
    "    from ast import literal_eval\n",
    "\n",
    "    topic_words = input\n",
    "\n",
    "    #for x in input:\n",
    "    #    topic_words.append((literal_eval(x)))\n",
    "\n",
    "    # change the list containing a list of elements into list of strings\n",
    "    topic_strings = []\n",
    "\n",
    "    for i in topic_words:\n",
    "        temp = ' '.join(i)\n",
    "        topic_strings.append(temp)\n",
    "\n",
    "    topic_string = ' '.join(topic_strings)\n",
    "\n",
    "    topic_string = [topic_string]\n",
    "\n",
    "    return topic_string\n",
    "\n",
    "def tokenization(input):\n",
    "    \n",
    "    # Load the tokenizer from the file\n",
    "    with open('posts1352_standard_GRU_tokenizer_2025_2.pickle', 'rb') as tokenizer_path:\n",
    "        loaded_tokenizer = pickle.load(tokenizer_path)\n",
    "\n",
    "    sequences = loaded_tokenizer.texts_to_sequences(input)\n",
    "\n",
    "    padded_data = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "    return padded_data\n",
    "\n",
    "def prediction(input):\n",
    "\n",
    "    model = tf.keras.models.load_model('posts1352_standard_GRU_model_2025_2.keras')\n",
    "    #model = tf.keras.models.load_model('posts1352_standard_GRU_model.h5',compile=False)\n",
    "    #model.compile() \n",
    "\n",
    "    input = change_to_string(input)\n",
    "\n",
    "    input = tokenization(input)\n",
    "\n",
    "    predictions = model.predict(input)\n",
    "\n",
    "    binary_predictions = (predictions > 0.5).astype(int)\n",
    "\n",
    "    return binary_predictions[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load UI.py\n",
    "import tkinter as tk\n",
    "from preprocess import preprocess\n",
    "from prediction import prediction\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "# Function to display the input string\n",
    "def show_output():\n",
    "    input_string = entry.get()\n",
    "    preprocessed_input_string = preprocess(input_string)\n",
    "    hate_speech_value = prediction(preprocessed_input_string)\n",
    "    if (hate_speech_value==1):\n",
    "        result = 'hate speech'\n",
    "    else:\n",
    "        result = 'not hate speech'\n",
    "    output_label.config(text=f\"This message is {result}.\")\n",
    "\n",
    "# Create the main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Message Detection System\")\n",
    "\n",
    "# Create and place the input label\n",
    "label = tk.Label(root, text=\"Submit any message:\")\n",
    "label.pack(pady=10)\n",
    "\n",
    "# Create and place the entry widget\n",
    "entry = tk.Entry(root, width=50)\n",
    "entry.pack(pady=10)\n",
    "\n",
    "# Create and place the submit button\n",
    "button = tk.Button(root, text=\"Submit\", command=show_output)\n",
    "button.pack(pady=10)\n",
    "\n",
    "# Create and place the output label\n",
    "output_label = tk.Label(root, text=\"\")\n",
    "output_label.pack(pady=10)\n",
    "\n",
    "# Run the Tkinter event loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPUIWytC3qqpbmxRXGXgLi2",
   "mount_file_id": "1i5zGNWJnMJlHQH-UYDDml7utfwsM_wVd",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
